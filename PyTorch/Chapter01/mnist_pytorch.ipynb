{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training and inference routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # us argmax to get the most likely prediction\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset),\n",
    "        100. * success / len(test_dataloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 30985698.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 77009341.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 27457960.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 10861190.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
    "    batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,)) \n",
    "                   ])),\n",
    "    batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define optimizer and run training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdurrahman/Library/Python/3.11/lib/python/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.301478\n",
      "epoch: 1 [320/60000 (1%)]\t training loss: 1.744134\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 1.240408\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 0.842911\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 0.949036\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 0.558535\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.620242\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.507009\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 0.467790\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.332347\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.371715\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.364097\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.332301\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.452664\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.365426\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.460753\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.203205\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.311852\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.120139\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.348500\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.240864\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.086965\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.311653\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.069089\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.228319\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.209495\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 0.335217\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.088607\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.077267\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.299731\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.217775\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.224393\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.202607\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.534872\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.091350\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.164567\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.305187\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.055809\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.157737\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.047026\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.132636\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.145910\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.079548\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.057228\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.036533\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.147256\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.132342\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.087954\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.199888\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.228492\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.152473\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.080728\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.041752\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.116074\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 0.051243\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.073239\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.040978\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.021529\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.074976\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.302484\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.146732\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.156637\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.089812\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.086282\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.077581\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.084695\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.348953\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.101311\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.152512\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.053798\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.205589\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.228144\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.036137\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.256782\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.358061\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.069725\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.060854\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.192327\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.066712\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.030303\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.317986\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 0.531736\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.031080\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.043352\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.229293\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.053658\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.012724\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.127009\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.007836\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.171291\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.102446\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.493341\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.014101\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.071718\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 0.399044\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.039880\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.101556\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.075627\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.053511\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.028118\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.002929\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.078423\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.211911\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.005935\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.481908\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.074895\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.049652\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.042982\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.005306\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.075362\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.119033\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.103477\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.049465\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.012482\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.071684\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.164245\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.214892\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.119813\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.012564\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.119609\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.123996\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 0.279660\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.018233\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.273742\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.048702\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.080287\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.056409\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.156436\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.140869\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.297263\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.170574\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.019871\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.008991\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.017744\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.101491\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.041510\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.151213\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.109483\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.144559\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.061091\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.304672\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.099090\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.169923\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.011046\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.440497\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.086487\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.176612\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.101528\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.100468\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 0.090491\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.124091\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.201857\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.009461\n",
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.011842\n",
      "epoch: 1 [49280/60000 (82%)]\t training loss: 0.182928\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.069349\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.010657\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.096470\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.003839\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.006649\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.236572\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.014727\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.024024\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.018086\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.035340\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.073316\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.027918\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.106017\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.128013\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.023136\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.114827\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.076415\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.002461\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.106343\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.019520\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.119551\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.035312\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.090167\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.027890\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.072046\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.048883\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.044264\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.033600\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.031268\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.054833\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.004903\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.020394\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.023926\n",
      "\n",
      "Test dataset: Overall Loss: 0.0484, Overall Accuracy: 9837/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.098174\n",
      "epoch: 2 [320/60000 (1%)]\t training loss: 0.023703\n",
      "epoch: 2 [640/60000 (1%)]\t training loss: 0.153959\n",
      "epoch: 2 [960/60000 (2%)]\t training loss: 0.136560\n",
      "epoch: 2 [1280/60000 (2%)]\t training loss: 0.049262\n",
      "epoch: 2 [1600/60000 (3%)]\t training loss: 0.021965\n",
      "epoch: 2 [1920/60000 (3%)]\t training loss: 0.027802\n",
      "epoch: 2 [2240/60000 (4%)]\t training loss: 0.003464\n",
      "epoch: 2 [2560/60000 (4%)]\t training loss: 0.006632\n",
      "epoch: 2 [2880/60000 (5%)]\t training loss: 0.077491\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.009212\n",
      "epoch: 2 [3520/60000 (6%)]\t training loss: 0.162178\n",
      "epoch: 2 [3840/60000 (6%)]\t training loss: 0.057268\n",
      "epoch: 2 [4160/60000 (7%)]\t training loss: 0.165004\n",
      "epoch: 2 [4480/60000 (7%)]\t training loss: 0.000849\n",
      "epoch: 2 [4800/60000 (8%)]\t training loss: 0.055694\n",
      "epoch: 2 [5120/60000 (9%)]\t training loss: 0.187408\n",
      "epoch: 2 [5440/60000 (9%)]\t training loss: 0.154628\n",
      "epoch: 2 [5760/60000 (10%)]\t training loss: 0.009212\n",
      "epoch: 2 [6080/60000 (10%)]\t training loss: 0.171722\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.079590\n",
      "epoch: 2 [6720/60000 (11%)]\t training loss: 0.163987\n",
      "epoch: 2 [7040/60000 (12%)]\t training loss: 0.090248\n",
      "epoch: 2 [7360/60000 (12%)]\t training loss: 0.040397\n",
      "epoch: 2 [7680/60000 (13%)]\t training loss: 0.077675\n",
      "epoch: 2 [8000/60000 (13%)]\t training loss: 0.027236\n",
      "epoch: 2 [8320/60000 (14%)]\t training loss: 0.035142\n",
      "epoch: 2 [8640/60000 (14%)]\t training loss: 0.091843\n",
      "epoch: 2 [8960/60000 (15%)]\t training loss: 0.440039\n",
      "epoch: 2 [9280/60000 (15%)]\t training loss: 0.086367\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.024360\n",
      "epoch: 2 [9920/60000 (17%)]\t training loss: 0.006449\n",
      "epoch: 2 [10240/60000 (17%)]\t training loss: 0.010307\n",
      "epoch: 2 [10560/60000 (18%)]\t training loss: 0.107418\n",
      "epoch: 2 [10880/60000 (18%)]\t training loss: 0.022558\n",
      "epoch: 2 [11200/60000 (19%)]\t training loss: 0.045512\n",
      "epoch: 2 [11520/60000 (19%)]\t training loss: 0.035157\n",
      "epoch: 2 [11840/60000 (20%)]\t training loss: 0.014696\n",
      "epoch: 2 [12160/60000 (20%)]\t training loss: 0.016473\n",
      "epoch: 2 [12480/60000 (21%)]\t training loss: 0.039737\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.049250\n",
      "epoch: 2 [13120/60000 (22%)]\t training loss: 0.157390\n",
      "epoch: 2 [13440/60000 (22%)]\t training loss: 0.018329\n",
      "epoch: 2 [13760/60000 (23%)]\t training loss: 0.014318\n",
      "epoch: 2 [14080/60000 (23%)]\t training loss: 0.052196\n",
      "epoch: 2 [14400/60000 (24%)]\t training loss: 0.034998\n",
      "epoch: 2 [14720/60000 (25%)]\t training loss: 0.043726\n",
      "epoch: 2 [15040/60000 (25%)]\t training loss: 0.037307\n",
      "epoch: 2 [15360/60000 (26%)]\t training loss: 0.126129\n",
      "epoch: 2 [15680/60000 (26%)]\t training loss: 0.014008\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.017495\n",
      "epoch: 2 [16320/60000 (27%)]\t training loss: 0.027524\n",
      "epoch: 2 [16640/60000 (28%)]\t training loss: 0.080501\n",
      "epoch: 2 [16960/60000 (28%)]\t training loss: 0.011050\n",
      "epoch: 2 [17280/60000 (29%)]\t training loss: 0.060049\n",
      "epoch: 2 [17600/60000 (29%)]\t training loss: 0.101983\n",
      "epoch: 2 [17920/60000 (30%)]\t training loss: 0.000665\n",
      "epoch: 2 [18240/60000 (30%)]\t training loss: 0.074447\n",
      "epoch: 2 [18560/60000 (31%)]\t training loss: 0.013062\n",
      "epoch: 2 [18880/60000 (31%)]\t training loss: 0.014776\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.010192\n",
      "epoch: 2 [19520/60000 (33%)]\t training loss: 0.009810\n",
      "epoch: 2 [19840/60000 (33%)]\t training loss: 0.169672\n",
      "epoch: 2 [20160/60000 (34%)]\t training loss: 0.012401\n",
      "epoch: 2 [20480/60000 (34%)]\t training loss: 0.004820\n",
      "epoch: 2 [20800/60000 (35%)]\t training loss: 0.015489\n",
      "epoch: 2 [21120/60000 (35%)]\t training loss: 0.077651\n",
      "epoch: 2 [21440/60000 (36%)]\t training loss: 0.033374\n",
      "epoch: 2 [21760/60000 (36%)]\t training loss: 0.022966\n",
      "epoch: 2 [22080/60000 (37%)]\t training loss: 0.073334\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.000803\n",
      "epoch: 2 [22720/60000 (38%)]\t training loss: 0.006425\n",
      "epoch: 2 [23040/60000 (38%)]\t training loss: 0.016063\n",
      "epoch: 2 [23360/60000 (39%)]\t training loss: 0.004546\n",
      "epoch: 2 [23680/60000 (39%)]\t training loss: 0.003831\n",
      "epoch: 2 [24000/60000 (40%)]\t training loss: 0.047946\n",
      "epoch: 2 [24320/60000 (41%)]\t training loss: 0.002812\n",
      "epoch: 2 [24640/60000 (41%)]\t training loss: 0.032683\n",
      "epoch: 2 [24960/60000 (42%)]\t training loss: 0.079832\n",
      "epoch: 2 [25280/60000 (42%)]\t training loss: 0.015188\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.009036\n",
      "epoch: 2 [25920/60000 (43%)]\t training loss: 0.002251\n",
      "epoch: 2 [26240/60000 (44%)]\t training loss: 0.108962\n",
      "epoch: 2 [26560/60000 (44%)]\t training loss: 0.073499\n",
      "epoch: 2 [26880/60000 (45%)]\t training loss: 0.065956\n",
      "epoch: 2 [27200/60000 (45%)]\t training loss: 0.008357\n",
      "epoch: 2 [27520/60000 (46%)]\t training loss: 0.007351\n",
      "epoch: 2 [27840/60000 (46%)]\t training loss: 0.033639\n",
      "epoch: 2 [28160/60000 (47%)]\t training loss: 0.153226\n",
      "epoch: 2 [28480/60000 (47%)]\t training loss: 0.044676\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.011117\n",
      "epoch: 2 [29120/60000 (49%)]\t training loss: 0.074001\n",
      "epoch: 2 [29440/60000 (49%)]\t training loss: 0.193947\n",
      "epoch: 2 [29760/60000 (50%)]\t training loss: 0.043343\n",
      "epoch: 2 [30080/60000 (50%)]\t training loss: 0.002562\n",
      "epoch: 2 [30400/60000 (51%)]\t training loss: 0.042341\n",
      "epoch: 2 [30720/60000 (51%)]\t training loss: 0.373437\n",
      "epoch: 2 [31040/60000 (52%)]\t training loss: 0.007059\n",
      "epoch: 2 [31360/60000 (52%)]\t training loss: 0.028347\n",
      "epoch: 2 [31680/60000 (53%)]\t training loss: 0.066970\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.201568\n",
      "epoch: 2 [32320/60000 (54%)]\t training loss: 0.024534\n",
      "epoch: 2 [32640/60000 (54%)]\t training loss: 0.104064\n",
      "epoch: 2 [32960/60000 (55%)]\t training loss: 0.009888\n",
      "epoch: 2 [33280/60000 (55%)]\t training loss: 0.007596\n",
      "epoch: 2 [33600/60000 (56%)]\t training loss: 0.397131\n",
      "epoch: 2 [33920/60000 (57%)]\t training loss: 0.070473\n",
      "epoch: 2 [34240/60000 (57%)]\t training loss: 0.033654\n",
      "epoch: 2 [34560/60000 (58%)]\t training loss: 0.019546\n",
      "epoch: 2 [34880/60000 (58%)]\t training loss: 0.009284\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.035103\n",
      "epoch: 2 [35520/60000 (59%)]\t training loss: 0.008294\n",
      "epoch: 2 [35840/60000 (60%)]\t training loss: 0.007669\n",
      "epoch: 2 [36160/60000 (60%)]\t training loss: 0.095953\n",
      "epoch: 2 [36480/60000 (61%)]\t training loss: 0.079869\n",
      "epoch: 2 [36800/60000 (61%)]\t training loss: 0.037694\n",
      "epoch: 2 [37120/60000 (62%)]\t training loss: 0.267672\n",
      "epoch: 2 [37440/60000 (62%)]\t training loss: 0.108975\n",
      "epoch: 2 [37760/60000 (63%)]\t training loss: 0.139832\n",
      "epoch: 2 [38080/60000 (63%)]\t training loss: 0.096193\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.117113\n",
      "epoch: 2 [38720/60000 (65%)]\t training loss: 0.014235\n",
      "epoch: 2 [39040/60000 (65%)]\t training loss: 0.014314\n",
      "epoch: 2 [39360/60000 (66%)]\t training loss: 0.034373\n",
      "epoch: 2 [39680/60000 (66%)]\t training loss: 0.001104\n",
      "epoch: 2 [40000/60000 (67%)]\t training loss: 0.147021\n",
      "epoch: 2 [40320/60000 (67%)]\t training loss: 0.058104\n",
      "epoch: 2 [40640/60000 (68%)]\t training loss: 0.012090\n",
      "epoch: 2 [40960/60000 (68%)]\t training loss: 0.002220\n",
      "epoch: 2 [41280/60000 (69%)]\t training loss: 0.024926\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.018475\n",
      "epoch: 2 [41920/60000 (70%)]\t training loss: 0.002290\n",
      "epoch: 2 [42240/60000 (70%)]\t training loss: 0.000232\n",
      "epoch: 2 [42560/60000 (71%)]\t training loss: 0.014631\n",
      "epoch: 2 [42880/60000 (71%)]\t training loss: 0.024298\n",
      "epoch: 2 [43200/60000 (72%)]\t training loss: 0.030139\n",
      "epoch: 2 [43520/60000 (73%)]\t training loss: 0.087239\n",
      "epoch: 2 [43840/60000 (73%)]\t training loss: 0.007357\n",
      "epoch: 2 [44160/60000 (74%)]\t training loss: 0.150379\n",
      "epoch: 2 [44480/60000 (74%)]\t training loss: 0.020359\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.015349\n",
      "epoch: 2 [45120/60000 (75%)]\t training loss: 0.002177\n",
      "epoch: 2 [45440/60000 (76%)]\t training loss: 0.148906\n",
      "epoch: 2 [45760/60000 (76%)]\t training loss: 0.134503\n",
      "epoch: 2 [46080/60000 (77%)]\t training loss: 0.170075\n",
      "epoch: 2 [46400/60000 (77%)]\t training loss: 0.262906\n",
      "epoch: 2 [46720/60000 (78%)]\t training loss: 0.034090\n",
      "epoch: 2 [47040/60000 (78%)]\t training loss: 0.004239\n",
      "epoch: 2 [47360/60000 (79%)]\t training loss: 0.013591\n",
      "epoch: 2 [47680/60000 (79%)]\t training loss: 0.131799\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.221314\n",
      "epoch: 2 [48320/60000 (81%)]\t training loss: 0.021087\n",
      "epoch: 2 [48640/60000 (81%)]\t training loss: 0.016724\n",
      "epoch: 2 [48960/60000 (82%)]\t training loss: 0.163884\n",
      "epoch: 2 [49280/60000 (82%)]\t training loss: 0.095761\n",
      "epoch: 2 [49600/60000 (83%)]\t training loss: 0.103023\n",
      "epoch: 2 [49920/60000 (83%)]\t training loss: 0.031056\n",
      "epoch: 2 [50240/60000 (84%)]\t training loss: 0.066328\n",
      "epoch: 2 [50560/60000 (84%)]\t training loss: 0.008336\n",
      "epoch: 2 [50880/60000 (85%)]\t training loss: 0.373142\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.002989\n",
      "epoch: 2 [51520/60000 (86%)]\t training loss: 0.041172\n",
      "epoch: 2 [51840/60000 (86%)]\t training loss: 0.500364\n",
      "epoch: 2 [52160/60000 (87%)]\t training loss: 0.022897\n",
      "epoch: 2 [52480/60000 (87%)]\t training loss: 0.046868\n",
      "epoch: 2 [52800/60000 (88%)]\t training loss: 0.269853\n",
      "epoch: 2 [53120/60000 (89%)]\t training loss: 0.009009\n",
      "epoch: 2 [53440/60000 (89%)]\t training loss: 0.136329\n",
      "epoch: 2 [53760/60000 (90%)]\t training loss: 0.073011\n",
      "epoch: 2 [54080/60000 (90%)]\t training loss: 0.025033\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 0.156014\n",
      "epoch: 2 [54720/60000 (91%)]\t training loss: 0.365546\n",
      "epoch: 2 [55040/60000 (92%)]\t training loss: 0.031742\n",
      "epoch: 2 [55360/60000 (92%)]\t training loss: 0.015810\n",
      "epoch: 2 [55680/60000 (93%)]\t training loss: 0.236210\n",
      "epoch: 2 [56000/60000 (93%)]\t training loss: 0.032649\n",
      "epoch: 2 [56320/60000 (94%)]\t training loss: 0.189793\n",
      "epoch: 2 [56640/60000 (94%)]\t training loss: 0.456181\n",
      "epoch: 2 [56960/60000 (95%)]\t training loss: 0.004161\n",
      "epoch: 2 [57280/60000 (95%)]\t training loss: 0.021010\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.122520\n",
      "epoch: 2 [57920/60000 (97%)]\t training loss: 0.014843\n",
      "epoch: 2 [58240/60000 (97%)]\t training loss: 0.077786\n",
      "epoch: 2 [58560/60000 (98%)]\t training loss: 0.008636\n",
      "epoch: 2 [58880/60000 (98%)]\t training loss: 0.004934\n",
      "epoch: 2 [59200/60000 (99%)]\t training loss: 0.211395\n",
      "epoch: 2 [59520/60000 (99%)]\t training loss: 0.087741\n",
      "epoch: 2 [59840/60000 (100%)]\t training loss: 0.001973\n",
      "\n",
      "Test dataset: Overall Loss: 0.0408, Overall Accuracy: 9854/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run inference on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "\n",
    "plt.imshow(sample_data[3][0], cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is : 0\n",
      "Ground truth is : 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][3]}\")\n",
    "print(f\"Ground truth is : {sample_targets[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
