{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Epsilon, Total number of possible actions\n",
    "epsilon = 1.0\n",
    "n_actions = 64\n",
    "\n",
    "#ε-greedy policy (add a randomness ε for the choice of the action)\n",
    "def selectEpsilonAction(table, obs, n_actions):\n",
    "    value, action = bestActionValue(table, obs)\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_actions-1)\n",
    "    else:\n",
    "        return action\n",
    "\n",
    "#greedy policy (take the best action according to the policy)\n",
    "def selectGreedyAction(table, obs, n_actions):\n",
    "    value, action = bestActionValue(table, obs)\n",
    "    return action\n",
    "\n",
    "#Explore the table, To find the best action that maximises Q(s,a)\n",
    "def bestActionValue(table, state):\n",
    "    bestAction = 0\n",
    "    maxValue = 0\n",
    "    for action in range(n_actions):\n",
    "        if table[(state, action)] > maxValue:\n",
    "            bestAction = action\n",
    "            maxValue = table[(state, action)]\n",
    "\n",
    "    return maxValue, bestAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Gamma and Learning Rate\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.8\n",
    "\n",
    "\n",
    "#To Update Q(obs0,action) according to Q(obs1,*) and the reward obtained.\n",
    "def QLearning(table, obs0, obs1, reward, action):\n",
    "\n",
    "    bestValue = bestActionValue(table, obs1)\n",
    "    QTarget = reward + GAMMA * bestValue\n",
    "    QError = QTarget - table[(obs0, action)]\n",
    "    table[(obs0, action)] += LEARNING_RATE * QError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Test Epidosodes\n",
    "TEST_EPISODES = 100\n",
    "#Test Game Loop\n",
    "def testGame(env, table):\n",
    "    n_actions = env.actionSpace.n\n",
    "    rewardGames = []\n",
    "    for _ in range(TEST_EPISODES):\n",
    "        obs = env.reset()\n",
    "        rewards = 0\n",
    "        while True:\n",
    "            #Act Greedly\n",
    "            next_obs, reward, done, _ = env.step(selectGreedyAction(table, obs, n_actions))\n",
    "            obs = next_obs\n",
    "            rewards += reward\n",
    "            if done:\n",
    "                rewardGames.append(rewards)\n",
    "                break\n",
    "    return np.mean(rewardGames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 30\u001b[0m\n\u001b[1;32m     25\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m gamesCount \u001b[38;5;241m<\u001b[39m MAX_GAMES:\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#Select the action followign an ε-greedy policy\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mselectEpsilonAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     next_obs, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#Update the Q-Table\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m, in \u001b[0;36mselectEpsilonAction\u001b[0;34m(table, obs, n_actions)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselectEpsilonAction\u001b[39m(table, obs, n_actions):\n\u001b[0;32m----> 7\u001b[0m     value, action \u001b[38;5;241m=\u001b[39m \u001b[43mbestActionValue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m epsilon:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, n_actions\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36mbestActionValue\u001b[0;34m(table, state)\u001b[0m\n\u001b[1;32m     22\u001b[0m maxValue \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_actions):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m>\u001b[39m maxValue:\n\u001b[1;32m     25\u001b[0m         bestAction \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m     26\u001b[0m         maxValue \u001b[38;5;241m=\u001b[39m table[(state, action)]\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Define Max Games and Epsilon Decay Rate\n",
    "MAX_GAMES = 15000\n",
    "EPS_DECAY_RATE = 0.9993\n",
    "\n",
    "#Create Frozen Lake Terrain\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "obs =  env.reset()\n",
    "\n",
    "obsLength = env.observation_space.n\n",
    "n_actions =  env.action_space.n\n",
    "\n",
    "rewardCount = 0\n",
    "gamesCount = 0\n",
    "\n",
    "#Initialize the table with Nil values\n",
    "table = collections.defaultdict(float)\n",
    "testRewardList = []\n",
    "\n",
    "#Reinitialize epsilon after each session\n",
    "epsilon = 1.0\n",
    "\n",
    "while gamesCount < MAX_GAMES:\n",
    "\n",
    "    #Select the action followign an ε-greedy policy\n",
    "    action = selectEpsilonAction(table, obs, n_actions)\n",
    "    next_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "    #Update the Q-Table\n",
    "    QLearning(table, obs, next_obs, reward, action) \n",
    "\n",
    "    rewardCount += reward\n",
    "    obs = next_obs\n",
    "\n",
    "    if done:\n",
    "        epsilon *= EPS_DECAY_RATE\n",
    "\n",
    "        #Test the new Q-Table every 1000 Games\n",
    "        if (gamesCount +1) % 1000 == 0:\n",
    "            testReward = testGame(env, table)\n",
    "            print('\\tGame Count:', gamesCount, \"\\tTest Reward:\", testReward, \"Epsilon Value:\", np,round(epsilon,2))\n",
    "\n",
    "            testRewardList.append(testReward)\n",
    "        \n",
    "        obs = env.reset()\n",
    "        rewardCount = 0\n",
    "        gamesCount +=1\n",
    "\n",
    "#Plot the Accuracy over the Number of Steps\n",
    "plt.figure(figsize=(18,9))\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(testRewardList)\n",
    "plt.show()        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
